{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Cloud Analysis with Softmax (LOG SCALE)\n",
    "## Advanced Technical Analysis for Stock Market Convergence Detection\n",
    "\n",
    "This notebook implements the trend cloud algorithm using **logarithmic scale** for better trend analysis:\n",
    "1. **Data Collection**: Get 1 year of stock candle data\n",
    "2. **Log Transformation**: Convert prices to log scale for percentage-based analysis\n",
    "3. **Pivot Detection**: Find swing highs and lows on log scale\n",
    "4. **Trendline Detection**: Find top 20 log-scale trendlines through most pivot points\n",
    "5. **Projection**: Project log-scale trendlines 5 days ahead\n",
    "6. **Convergence Detection**: Find where log-scale trendlines converge\n",
    "7. **Binning**: Group convergences into price bins\n",
    "8. **Softmax**: Apply softmax to emphasize strongest convergence zones\n",
    "9. **Visualization**: Create trend clouds with log-scale plotting\n",
    "\n",
    "**ðŸ”¬ LOG SCALE ADVANTAGES:**\n",
    "- Trendlines represent **constant percentage growth rates** (e.g., +5%/day)\n",
    "- Better handling of exponential price movements\n",
    "- More meaningful for long-term trend analysis\n",
    "- Percentage-based distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:18.901707Z",
     "iopub.status.busy": "2025-09-04T03:30:18.901555Z",
     "iopub.status.idle": "2025-09-04T03:30:20.410790Z",
     "shell.execute_reply": "2025-09-04T03:30:20.410512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from scipy.signal import argrelextrema\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting with WHITE background\n",
    "plt.style.use('default')  # Changed from 'dark_background' to white\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['text.color'] = 'black'\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully with WHITE background theme!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:20.430960Z",
     "iopub.status.busy": "2025-09-04T03:30:20.430744Z",
     "iopub.status.idle": "2025-09-04T03:30:20.434826Z",
     "shell.execute_reply": "2025-09-04T03:30:20.434509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - FIRST WINDOW VALIDATION MODE\n",
    "SYMBOL = 'QQQ'  # Change this to test different stocks: QQQ, AAPL, MSFT, GOOGL, TSLA\n",
    "\n",
    "# ðŸŽ¯ FIRST WINDOW VALIDATION: Use same period as W000 from fixed analyzer\n",
    "VALIDATION_MODE = True\n",
    "WINDOW_START = '2023-10-17'  # Start date for first window (250 trading days back)\n",
    "WINDOW_END = '2024-10-04'    # End date for first window\n",
    "EXPECTED_PRICE = 89.16       # Expected closing price from analyzer\n",
    "EXPECTED_CLUSTERS = 8        # Expected number of clusters from analyzer\n",
    "\n",
    "# Calculate lookback days dynamically for first window\n",
    "from datetime import datetime\n",
    "if VALIDATION_MODE:\n",
    "    start_date = datetime.strptime(WINDOW_START, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(WINDOW_END, '%Y-%m-%d')\n",
    "    LOOKBACK_DAYS = (end_date - start_date).days + 50  # Add buffer for weekends/holidays\n",
    "else:\n",
    "    LOOKBACK_DAYS = 365  # Default 1 year of data\n",
    "\n",
    "PROJECTION_DAYS = 5  # Project 5 days ahead (same as analyzer)\n",
    "MAX_TRENDLINES = 30  # Top 30 trendlines\n",
    "CONVERGENCE_THRESHOLD = 0.05  # 5% price threshold for convergence\n",
    "TEMPERATURE = 2.0  # Softmax temperature (same as analyzer)\n",
    "\n",
    "print(f\"ðŸŽ¯ Configuration:\")\n",
    "if VALIDATION_MODE:\n",
    "    print(f\"   ðŸ” VALIDATION MODE: First Window (W000)\")\n",
    "    print(f\"   ðŸ“… Period: {WINDOW_START} to {WINDOW_END}\")\n",
    "    print(f\"   ðŸ’° Expected final price: ${EXPECTED_PRICE}\")\n",
    "    print(f\"   ðŸŽª Expected clusters: {EXPECTED_CLUSTERS}\")\n",
    "    print(f\"   ðŸ“Š Lookback days: {LOOKBACK_DAYS} (calculated for window)\")\n",
    "else:\n",
    "    print(f\"   ðŸ“Š Normal mode: {LOOKBACK_DAYS} days lookback\")\n",
    "\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "print(f\"   Projection: {PROJECTION_DAYS} days ahead\")\n",
    "print(f\"   Max trendlines: {MAX_TRENDLINES}\")\n",
    "print(f\"   Convergence threshold: {CONVERGENCE_THRESHOLD*100}%\")\n",
    "print(f\"   Temperature: {TEMPERATURE}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ LOG SCALE Analysis Configuration:\")\n",
    "print(f\"   â€¢ Trendlines represent constant percentage growth rates\")\n",
    "print(f\"   â€¢ Better handling of exponential price movements\")\n",
    "print(f\"   â€¢ More meaningful for long-term trend analysis\")\n",
    "print(f\"   â€¢ 2% tolerance properly converted to log space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:20.436935Z",
     "iopub.status.busy": "2025-09-04T03:30:20.436835Z",
     "iopub.status.idle": "2025-09-04T03:30:20.566230Z",
     "shell.execute_reply": "2025-09-04T03:30:20.565884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load stock data for FIRST WINDOW VALIDATION + LOG TRANSFORMATION\n",
    "import sqlite3\n",
    "\n",
    "def load_stock_data_from_db(symbol, days=365, timeframe='1D', filter_premarket=True, validation_mode=False, window_start=None, window_end=None):\n",
    "    \"\"\"Load stock data from the local SQLite database, with first window validation support\"\"\"\n",
    "    db_path = 'data/stock-data.db'\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Query with correct column names and timeframe case\n",
    "        query = \"\"\"\n",
    "        SELECT timestamp, open, high, low, close, volume, adjusted_close\n",
    "        FROM market_data\n",
    "        WHERE symbol = ? AND timeframe = ?\n",
    "        ORDER BY timestamp ASC\n",
    "        \"\"\"\n",
    "\n",
    "        if validation_mode and window_start and window_end:\n",
    "            print(f\"ðŸ“Š ðŸŽ¯ VALIDATION MODE: Loading {symbol} data for first window analysis\")\n",
    "            print(f\"   ðŸ“… Target period: {window_start} to {window_end}\")\n",
    "        else:\n",
    "            print(f\"ðŸ“Š Loading {symbol} data from local database (timeframe: {timeframe})...\")\n",
    "\n",
    "        # Get more data for validation mode to ensure we have the exact window\n",
    "        data_limit = days * 5 if validation_mode else days * 2\n",
    "        df = pd.read_sql_query(query, conn, params=(symbol, timeframe))\n",
    "        conn.close()\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"âŒ No data found for {symbol} with timeframe {timeframe} in database\")\n",
    "            # Try alternative timeframe cases\n",
    "            for alt_timeframe in ['1d', '1D', 'daily', 'DAILY']:\n",
    "                if alt_timeframe != timeframe:\n",
    "                    print(f\"ðŸ”„ Trying alternative timeframe: {alt_timeframe}\")\n",
    "                    conn = sqlite3.connect(db_path)\n",
    "                    df = pd.read_sql_query(query, conn, params=(symbol, alt_timeframe))\n",
    "                    conn.close()\n",
    "                    if not df.empty:\n",
    "                        print(f\"âœ… Found data with timeframe: {alt_timeframe}\")\n",
    "                        timeframe = alt_timeframe\n",
    "                        break\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"âŒ No data found for {symbol} with any timeframe\")\n",
    "                return create_sample_data_validation(symbol, window_start, window_end) if validation_mode else create_sample_data(symbol, days)\n",
    "\n",
    "        # Convert timestamp and prepare data\n",
    "        df['Date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df = df.rename(columns={\n",
    "            'open': 'Open',\n",
    "            'high': 'High',\n",
    "            'low': 'Low',\n",
    "            'close': 'Close',\n",
    "            'volume': 'Volume'\n",
    "        })\n",
    "        df['Price'] = df['Close']  # Use closing price as main price\n",
    "\n",
    "        # Filter out premarket data if requested\n",
    "        if filter_premarket:\n",
    "            # Convert to Eastern Time (market timezone)\n",
    "            df['DateTime_ET'] = df['Date'].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "            df['Hour'] = df['DateTime_ET'].dt.hour\n",
    "            df['Minute'] = df['DateTime_ET'].dt.minute\n",
    "\n",
    "            print(f\"ðŸ“Š Before filtering: {len(df)} candles\")\n",
    "\n",
    "            # Filter for regular trading hours (9:30 AM - 4:00 PM ET)\n",
    "            regular_hours_mask = (\n",
    "                ((df['Hour'] == 9) & (df['Minute'] >= 30)) |  # 9:30 AM onwards\n",
    "                (df['Hour'].between(10, 15)) |                # 10 AM - 3:59 PM\n",
    "                ((df['Hour'] == 16) & (df['Minute'] == 0))    # 4:00 PM market close\n",
    "            )\n",
    "\n",
    "            # Also filter for weekdays only (Monday=0 to Friday=4)\n",
    "            df['DayOfWeek'] = df['DateTime_ET'].dt.dayofweek\n",
    "            weekday_mask = df['DayOfWeek'] < 5\n",
    "\n",
    "            # Combine filters\n",
    "            trading_hours_mask = regular_hours_mask & weekday_mask\n",
    "            df_filtered = df[trading_hours_mask].copy()\n",
    "\n",
    "            print(f\"ðŸ“Š After filtering: {len(df_filtered)} candles (removed {len(df) - len(df_filtered)} premarket/afterhours)\")\n",
    "\n",
    "            if len(df_filtered) == 0:\n",
    "                print(f\"âš ï¸ No regular trading hours data found, using all data\")\n",
    "                df_filtered = df\n",
    "\n",
    "            # Clean up temporary columns\n",
    "            df_filtered = df_filtered.drop(['DateTime_ET', 'Hour', 'Minute', 'DayOfWeek'], axis=1, errors='ignore')\n",
    "            df = df_filtered\n",
    "\n",
    "        # Sort by date (oldest first)\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "        # ðŸŽ¯ VALIDATION MODE: Filter to exact first window period\n",
    "        if validation_mode and window_start and window_end:\n",
    "            window_start_date = pd.to_datetime(window_start)\n",
    "            window_end_date = pd.to_datetime(window_end)\n",
    "\n",
    "            print(f\"ðŸŽ¯ Filtering to first window period...\")\n",
    "            print(f\"   Available data range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "\n",
    "            # Filter to exact window period\n",
    "            window_mask = (df['Date'] >= window_start_date) & (df['Date'] <= window_end_date)\n",
    "            window_data = df[window_mask].copy().reset_index(drop=True)\n",
    "\n",
    "            if len(window_data) == 0:\n",
    "                print(f\"âŒ No data found for window period {window_start} to {window_end}\")\n",
    "                print(\"   Using sample data for demonstration...\")\n",
    "                return create_sample_data_validation(symbol, window_start, window_end)\n",
    "\n",
    "            df = window_data\n",
    "            print(f\"âœ… Filtered to window period: {len(df)} trading days\")\n",
    "\n",
    "            # Validate final price matches expected\n",
    "            final_price = df['Price'].iloc[-1]\n",
    "            final_date = df['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "            print(f\"ðŸ“Š Window validation:\")\n",
    "            print(f\"   Final date: {final_date} (expected: {window_end})\")\n",
    "            print(f\"   Final price: ${final_price:.2f} (expected: ${EXPECTED_PRICE})\")\n",
    "\n",
    "            price_diff = abs(final_price - EXPECTED_PRICE)\n",
    "            if price_diff < 1.0:\n",
    "                print(f\"   âœ… Price validation: PASSED (difference: ${price_diff:.2f})\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ Price validation: Different than expected (difference: ${price_diff:.2f})\")\n",
    "                print(\"      This might be due to data source differences or adjustments\")\n",
    "\n",
    "        else:\n",
    "            # Normal mode: limit to requested days\n",
    "            df = df.tail(days)\n",
    "\n",
    "        # ðŸ”¬ ADD LOG TRANSFORMATION FOR LOG SCALE ANALYSIS\n",
    "        df['LogPrice'] = np.log(df['Price'])  # Natural log for percentage-based analysis\n",
    "\n",
    "        print(f\"âœ… Final dataset: {len(df)} candles for {symbol}\")\n",
    "        print(f\"   Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "        print(f\"   Price range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\")\n",
    "        print(f\"   ðŸ“ˆ LogPrice range: {df['LogPrice'].min():.4f} - {df['LogPrice'].max():.4f}\")\n",
    "        print(f\"   Current price: ${df['Price'].iloc[-1]:.2f} (log: {df['LogPrice'].iloc[-1]:.4f})\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading from database: {e}\")\n",
    "        if validation_mode:\n",
    "            print(f\"ðŸ”„ Creating sample data for first window validation...\")\n",
    "            return create_sample_data_validation(symbol, window_start, window_end)\n",
    "        else:\n",
    "            print(f\"ðŸ”„ Creating sample data for demonstration...\")\n",
    "            return create_sample_data(symbol, days)\n",
    "\n",
    "def create_sample_data_validation(symbol, window_start, window_end):\n",
    "    \"\"\"Create realistic sample stock data for first window validation\"\"\"\n",
    "    from datetime import timedelta\n",
    "\n",
    "    start_date = pd.to_datetime(window_start)\n",
    "    end_date = pd.to_datetime(window_end)\n",
    "\n",
    "    # Generate business days only\n",
    "    date_range = pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Create realistic price progression for QQQ in 1999\n",
    "    np.random.seed(hash(symbol + window_start) % 2**32)  # Consistent seed\n",
    "\n",
    "    # QQQ started around 1999, simulate dot-com boom period\n",
    "    start_price = 100.0  # Starting price\n",
    "    end_price = EXPECTED_PRICE  # Target end price\n",
    "\n",
    "    # Calculate required daily return to reach target\n",
    "    days_in_window = len(date_range)\n",
    "    daily_return = (end_price / start_price) ** (1 / days_in_window) - 1\n",
    "\n",
    "    prices = []\n",
    "    current_price = start_price\n",
    "\n",
    "    for i, date in enumerate(date_range):\n",
    "        # Add trend + volatility\n",
    "        trend_return = daily_return\n",
    "        volatility = np.random.normal(0, 0.02)  # 2% daily volatility\n",
    "\n",
    "        # Occasionally add larger moves (market events)\n",
    "        if np.random.random() < 0.05:  # 5% chance\n",
    "            volatility += np.random.normal(0, 0.03)\n",
    "\n",
    "        current_price *= (1 + trend_return + volatility)\n",
    "        current_price = max(10, current_price)  # Floor price\n",
    "\n",
    "        prices.append(current_price)\n",
    "\n",
    "    # Adjust final price to match expected\n",
    "    prices[-1] = EXPECTED_PRICE\n",
    "\n",
    "    # Create OHLC data\n",
    "    data = []\n",
    "    for i, (date, close) in enumerate(zip(date_range, prices)):\n",
    "        # Generate realistic OHLC around close price\n",
    "        volatility = close * 0.01  # 1% intraday volatility\n",
    "        open_price = close + np.random.normal(0, volatility * 0.5)\n",
    "        high = max(open_price, close) + abs(np.random.normal(0, volatility * 0.3))\n",
    "        low = min(open_price, close) - abs(np.random.normal(0, volatility * 0.3))\n",
    "\n",
    "        data.append({\n",
    "            'Date': pd.Timestamp(date.replace(hour=16, minute=0)),  # Market close time\n",
    "            'Open': round(max(10, open_price), 2),\n",
    "            'High': round(max(10, high), 2),\n",
    "            'Low': round(max(10, low), 2),\n",
    "            'Close': round(max(10, close), 2),\n",
    "            'Volume': int(np.random.normal(50000, 10000)),\n",
    "            'Price': round(max(10, close), 2)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # ðŸ”¬ ADD LOG TRANSFORMATION\n",
    "    df['LogPrice'] = np.log(df['Price'])\n",
    "\n",
    "    print(f\"âœ… Created validation sample data: {len(df)} candles for {symbol} (W000 period)\")\n",
    "    print(f\"   Period: {window_start} to {window_end}\")\n",
    "    print(f\"   Price: ${df['Price'].iloc[0]:.2f} â†’ ${df['Price'].iloc[-1]:.2f}\")\n",
    "    print(f\"   ðŸ“ˆ LogPrice: {df['LogPrice'].iloc[0]:.4f} â†’ {df['LogPrice'].iloc[-1]:.4f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_sample_data(symbol, days=365):\n",
    "    \"\"\"Create realistic sample stock data for demonstration\"\"\"\n",
    "    np.random.seed(hash(symbol) % 2**32)  # Seed based on symbol for consistency\n",
    "\n",
    "    # Base parameters for different stocks\n",
    "    stock_params = {\n",
    "        'QQQ': {'start_price': 350, 'volatility': 0.02, 'trend': 0.0001},\n",
    "        'AAPL': {'start_price': 175, 'volatility': 0.025, 'trend': 0.0002},\n",
    "        'MSFT': {'start_price': 320, 'volatility': 0.022, 'trend': 0.0001},\n",
    "        'GOOGL': {'start_price': 130, 'volatility': 0.028, 'trend': 0.0001},\n",
    "        'TSLA': {'start_price': 250, 'volatility': 0.04, 'trend': -0.0001},\n",
    "    }\n",
    "\n",
    "    params = stock_params.get(symbol, {'start_price': 100, 'volatility': 0.025, 'trend': 0})\n",
    "\n",
    "    # Generate dates for weekdays only during market hours\n",
    "    end_date = datetime.now()\n",
    "    dates = pd.date_range(start=end_date - timedelta(days=days*1.5), end=end_date, freq='D')\n",
    "    # Filter to weekdays only\n",
    "    dates = [d.replace(hour=16, minute=0) for d in dates if d.weekday() < 5]  # 4 PM market close\n",
    "    dates = dates[-days:]  # Take only the requested number of days\n",
    "\n",
    "    # Generate realistic price data using geometric Brownian motion\n",
    "    price = params['start_price']\n",
    "    prices = []\n",
    "\n",
    "    for i, date in enumerate(dates):\n",
    "        # Add trend and random walk\n",
    "        daily_return = params['trend'] + np.random.normal(0, params['volatility'])\n",
    "        price = price * (1 + daily_return)\n",
    "\n",
    "        # Add some larger moves occasionally (news events)\n",
    "        if np.random.random() < 0.05:  # 5% chance of bigger move\n",
    "            price = price * (1 + np.random.normal(0, params['volatility'] * 3))\n",
    "\n",
    "        prices.append(max(1, price))  # Ensure positive prices\n",
    "\n",
    "    # Create OHLC data\n",
    "    data = []\n",
    "    for i, (date, close) in enumerate(zip(dates, prices)):\n",
    "        # Create realistic OHLC from close price\n",
    "        volatility = params['volatility'] * close\n",
    "        open_price = close + np.random.normal(0, volatility * 0.5)\n",
    "        high = max(open_price, close) + abs(np.random.normal(0, volatility * 0.3))\n",
    "        low = min(open_price, close) - abs(np.random.normal(0, volatility * 0.3))\n",
    "\n",
    "        data.append({\n",
    "            'Date': date,\n",
    "            'Open': round(max(1, open_price), 2),\n",
    "            'High': round(max(1, high), 2),\n",
    "            'Low': round(max(1, low), 2),\n",
    "            'Close': round(max(1, close), 2),\n",
    "            'Volume': int(np.random.normal(1000000, 200000)),\n",
    "            'Price': round(max(1, close), 2)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # ðŸ”¬ ADD LOG TRANSFORMATION FOR SAMPLE DATA TOO\n",
    "    df['LogPrice'] = np.log(df['Price'])\n",
    "\n",
    "    print(f\"âœ… Created sample data: {len(df)} candles for {symbol} (market hours only)\")\n",
    "    print(f\"   Price range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\")\n",
    "    print(f\"   ðŸ“ˆ LogPrice range: {df['LogPrice'].min():.4f} - {df['LogPrice'].max():.4f}\")\n",
    "    print(f\"   Current price: ${df['Price'].iloc[-1]:.2f} (log: {df['LogPrice'].iloc[-1]:.4f})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Check what symbols and timeframes are available in database\n",
    "def check_database_contents():\n",
    "    \"\"\"Check what data is available in the database\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('data/stock-data.db')\n",
    "\n",
    "        # Get available symbols and timeframes\n",
    "        query = \"\"\"\n",
    "        SELECT symbol, timeframe, COUNT(*) as record_count,\n",
    "               MIN(timestamp) as earliest, MAX(timestamp) as latest\n",
    "        FROM market_data\n",
    "        GROUP BY symbol, timeframe\n",
    "        ORDER BY symbol, timeframe\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Convert timestamps to readable dates\n",
    "        df['earliest_date'] = pd.to_datetime(df['earliest'], unit='s').dt.date\n",
    "        df['latest_date'] = pd.to_datetime(df['latest'], unit='s').dt.date\n",
    "\n",
    "        print(\"ðŸ“‹ Database Contents:\")\n",
    "        print(df[['symbol', 'timeframe', 'record_count', 'earliest_date', 'latest_date']].to_string(index=False))\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking database: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check database contents first\n",
    "database_info = check_database_contents()\n",
    "\n",
    "# Load data from database with first window validation support\n",
    "if VALIDATION_MODE:\n",
    "    stock_data = load_stock_data_from_db(SYMBOL, LOOKBACK_DAYS, timeframe='1D',\n",
    "                                       filter_premarket=True, validation_mode=True,\n",
    "                                       window_start=WINDOW_START, window_end=WINDOW_END)\n",
    "else:\n",
    "    stock_data = load_stock_data_from_db(SYMBOL, LOOKBACK_DAYS, timeframe='1D', filter_premarket=True)\n",
    "\n",
    "print(f\"\\nðŸ”¬ LOG SCALE Data Preparation Complete!\")\n",
    "print(f\"   ðŸ“ˆ LogPrice column added for percentage-based analysis\")\n",
    "if VALIDATION_MODE:\n",
    "    print(f\"   ðŸŽ¯ Ready for first window validation against fixed analyzer results\")\n",
    "\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:20.568276Z",
     "iopub.status.busy": "2025-09-04T03:30:20.568098Z",
     "iopub.status.idle": "2025-09-04T03:30:21.112159Z",
     "shell.execute_reply": "2025-09-04T03:30:21.111838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Ultra-enhanced pivot point detection on LOG SCALE (fixed date handling)\n",
    "def detect_pivot_points_ultra_log(data, methods=['scipy', 'rolling', 'zigzag', 'fractal', 'slope', 'derivative'], combine=True):\n",
    "    \"\"\"Ultra-enhanced pivot detection with comprehensive methods ON LOG SCALE\"\"\"\n",
    "    log_prices = data['LogPrice'].values  # ðŸ”¬ USE LOG PRICES INSTEAD OF REGULAR PRICES\n",
    "    regular_prices = data['Price'].values  # Keep regular prices for display\n",
    "    dates = data['Date'].values\n",
    "\n",
    "    all_pivots = []\n",
    "\n",
    "    print(f\"ðŸ” Ultra-enhanced LOG SCALE pivot detection using methods: {methods}\")\n",
    "    print(f\"   ðŸ“ˆ Working with log prices: {log_prices.min():.4f} to {log_prices.max():.4f}\")\n",
    "\n",
    "    # Method 1: Scipy with multiple window sizes ON LOG SCALE\n",
    "    if 'scipy' in methods:\n",
    "        print(\"   ðŸ“Š Method 1: Scipy argrelextrema with multiple windows (LOG SCALE)\")\n",
    "        for window in [2, 3, 4, 5, 7, 10, 15]:  # Even more window sizes including very small ones\n",
    "            swing_highs = argrelextrema(log_prices, np.greater, order=window)[0]\n",
    "            swing_lows = argrelextrema(log_prices, np.less, order=window)[0]\n",
    "\n",
    "            for idx in swing_highs:\n",
    "                all_pivots.append({\n",
    "                    'date': pd.to_datetime(dates[idx]),  # Ensure datetime object\n",
    "                    'price': regular_prices[idx],        # Store regular price for display\n",
    "                    'log_price': log_prices[idx],        # Store log price for calculations\n",
    "                    'type': 'high',\n",
    "                    'index': idx,\n",
    "                    'method': f'scipy_w{window}',\n",
    "                    'strength': window\n",
    "                })\n",
    "\n",
    "            for idx in swing_lows:\n",
    "                all_pivots.append({\n",
    "                    'date': pd.to_datetime(dates[idx]),  # Ensure datetime object\n",
    "                    'price': regular_prices[idx],        # Store regular price for display\n",
    "                    'log_price': log_prices[idx],        # Store log price for calculations\n",
    "                    'type': 'low',\n",
    "                    'index': idx,\n",
    "                    'method': f'scipy_w{window}',\n",
    "                    'strength': window\n",
    "                })\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'scipy' in p['method']])} scipy pivots\")\n",
    "\n",
    "    # Method 2: Rolling window extremes ON LOG SCALE\n",
    "    if 'rolling' in methods:\n",
    "        print(\"   ðŸ“Š Method 2: Rolling window extremes (LOG SCALE)\")\n",
    "        for window in [3, 5, 7, 10, 15, 20]:\n",
    "            df_temp = pd.DataFrame({'log_price': log_prices, 'price': regular_prices, 'index': range(len(log_prices))})\n",
    "\n",
    "            rolling_max = df_temp['log_price'].rolling(window=window, center=True).max()\n",
    "            rolling_min = df_temp['log_price'].rolling(window=window, center=True).min()\n",
    "\n",
    "            highs = df_temp[(df_temp['log_price'] == rolling_max)]['index'].values\n",
    "            lows = df_temp[(df_temp['log_price'] == rolling_min)]['index'].values\n",
    "\n",
    "            for idx in highs:\n",
    "                if 0 < idx < len(log_prices) - 1:\n",
    "                    all_pivots.append({\n",
    "                        'date': pd.to_datetime(dates[idx]),  # Ensure datetime object\n",
    "                        'price': regular_prices[idx],        # Store regular price for display\n",
    "                        'log_price': log_prices[idx],        # Store log price for calculations\n",
    "                        'type': 'high',\n",
    "                        'index': idx,\n",
    "                        'method': f'rolling_w{window}',\n",
    "                        'strength': window / 3\n",
    "                    })\n",
    "\n",
    "            for idx in lows:\n",
    "                if 0 < idx < len(log_prices) - 1:\n",
    "                    all_pivots.append({\n",
    "                        'date': pd.to_datetime(dates[idx]),  # Ensure datetime object\n",
    "                        'price': regular_prices[idx],        # Store regular price for display\n",
    "                        'log_price': log_prices[idx],        # Store log price for calculations\n",
    "                        'type': 'low',\n",
    "                        'index': idx,\n",
    "                        'method': f'rolling_w{window}',\n",
    "                        'strength': window / 3\n",
    "                    })\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'rolling' in p['method']])} rolling pivots\")\n",
    "\n",
    "    # Method 3: ZigZag with multiple thresholds ON LOG SCALE\n",
    "    if 'zigzag' in methods:\n",
    "        print(\"   ðŸ“Š Method 3: ZigZag percentage-based detection (LOG SCALE)\")\n",
    "        for threshold in [0.01, 0.015, 0.02, 0.03, 0.05, 0.08]:  # Even smaller thresholds\n",
    "            zigzag_pivots = detect_zigzag_pivots_log(log_prices, regular_prices, dates, threshold)\n",
    "            for pivot in zigzag_pivots:\n",
    "                pivot['method'] = f'zigzag_{threshold*100:.1f}pct'\n",
    "                pivot['strength'] = 1 / threshold\n",
    "                pivot['date'] = pd.to_datetime(pivot['date'])  # Ensure datetime object\n",
    "                all_pivots.append(pivot)\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'zigzag' in p['method']])} zigzag pivots\")\n",
    "\n",
    "    # Method 4: Fractal-based detection ON LOG SCALE\n",
    "    if 'fractal' in methods:\n",
    "        print(\"   ðŸ“Š Method 4: Fractal pattern detection (LOG SCALE)\")\n",
    "        fractal_pivots = detect_fractal_pivots_log(log_prices, regular_prices, dates)\n",
    "        for pivot in fractal_pivots:\n",
    "            pivot['method'] = 'fractal'\n",
    "            pivot['strength'] = 3\n",
    "            pivot['date'] = pd.to_datetime(pivot['date'])  # Ensure datetime object\n",
    "            all_pivots.append(pivot)\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'fractal' in p['method']])} fractal pivots\")\n",
    "\n",
    "    # Method 5: Slope change detection ON LOG SCALE\n",
    "    if 'slope' in methods:\n",
    "        print(\"   ðŸ“Š Method 5: Slope change detection (LOG SCALE)\")\n",
    "        slope_pivots = detect_slope_change_pivots_log(log_prices, regular_prices, dates)\n",
    "        for pivot in slope_pivots:\n",
    "            pivot['method'] = 'slope'\n",
    "            pivot['strength'] = 2\n",
    "            pivot['date'] = pd.to_datetime(pivot['date'])  # Ensure datetime object\n",
    "            all_pivots.append(pivot)\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'slope' in p['method']])} slope pivots\")\n",
    "\n",
    "    # Method 6: Derivative-based detection ON LOG SCALE\n",
    "    if 'derivative' in methods:\n",
    "        print(\"   ðŸ“Š Method 6: Derivative-based detection (LOG SCALE)\")\n",
    "        derivative_pivots = detect_derivative_pivots_log(log_prices, regular_prices, dates)\n",
    "        for pivot in derivative_pivots:\n",
    "            pivot['method'] = 'derivative'\n",
    "            pivot['strength'] = 1.5\n",
    "            pivot['date'] = pd.to_datetime(pivot['date'])  # Ensure datetime object\n",
    "            all_pivots.append(pivot)\n",
    "\n",
    "        print(f\"      Found {len([p for p in all_pivots if 'derivative' in p['method']])} derivative pivots\")\n",
    "\n",
    "    print(f\"ðŸ” Total raw pivots found: {len(all_pivots)}\")\n",
    "\n",
    "    if combine and len(all_pivots) > 0:\n",
    "        combined_pivots = combine_overlapping_pivots(all_pivots, proximity_threshold=3)  # Tighter proximity\n",
    "        print(f\"ðŸ” Combined to {len(combined_pivots)} unique pivots\")\n",
    "        return combined_pivots, get_indices_by_type(combined_pivots, 'high'), get_indices_by_type(combined_pivots, 'low')\n",
    "    else:\n",
    "        return all_pivots, get_indices_by_type(all_pivots, 'high'), get_indices_by_type(all_pivots, 'low')\n",
    "\n",
    "def detect_fractal_pivots_log(log_prices, regular_prices, dates, lookback=2):\n",
    "    \"\"\"Detect fractal patterns (Williams Fractal) ON LOG SCALE\"\"\"\n",
    "    pivots = []\n",
    "\n",
    "    for i in range(lookback, len(log_prices) - lookback):\n",
    "        # Check for fractal high (higher than surrounding points) ON LOG SCALE\n",
    "        is_fractal_high = True\n",
    "        for j in range(i - lookback, i + lookback + 1):\n",
    "            if j != i and log_prices[j] >= log_prices[i]:\n",
    "                is_fractal_high = False\n",
    "                break\n",
    "\n",
    "        if is_fractal_high:\n",
    "            pivots.append({\n",
    "                'date': dates[i],\n",
    "                'price': regular_prices[i],\n",
    "                'log_price': log_prices[i],\n",
    "                'type': 'high',\n",
    "                'index': i\n",
    "            })\n",
    "\n",
    "        # Check for fractal low (lower than surrounding points) ON LOG SCALE\n",
    "        is_fractal_low = True\n",
    "        for j in range(i - lookback, i + lookback + 1):\n",
    "            if j != i and log_prices[j] <= log_prices[i]:\n",
    "                is_fractal_low = False\n",
    "                break\n",
    "\n",
    "        if is_fractal_low:\n",
    "            pivots.append({\n",
    "                'date': dates[i],\n",
    "                'price': regular_prices[i],\n",
    "                'log_price': log_prices[i],\n",
    "                'type': 'low',\n",
    "                'index': i\n",
    "            })\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def detect_slope_change_pivots_log(log_prices, regular_prices, dates, window=3):\n",
    "    \"\"\"Detect pivots based on slope changes ON LOG SCALE\"\"\"\n",
    "    pivots = []\n",
    "\n",
    "    # Calculate slopes ON LOG SCALE\n",
    "    slopes = []\n",
    "    for i in range(len(log_prices) - window):\n",
    "        slope = (log_prices[i + window] - log_prices[i]) / window\n",
    "        slopes.append(slope)\n",
    "\n",
    "    # Find slope changes\n",
    "    for i in range(1, len(slopes) - 1):\n",
    "        prev_slope = slopes[i - 1]\n",
    "        curr_slope = slopes[i]\n",
    "        next_slope = slopes[i + 1]\n",
    "\n",
    "        # Detect slope change from positive to negative (potential high)\n",
    "        if prev_slope > 0 and curr_slope < 0:\n",
    "            pivot_idx = i + window // 2\n",
    "            if 0 <= pivot_idx < len(log_prices):\n",
    "                pivots.append({\n",
    "                    'date': dates[pivot_idx],\n",
    "                    'price': regular_prices[pivot_idx],\n",
    "                    'log_price': log_prices[pivot_idx],\n",
    "                    'type': 'high',\n",
    "                    'index': pivot_idx\n",
    "                })\n",
    "\n",
    "        # Detect slope change from negative to positive (potential low)\n",
    "        elif prev_slope < 0 and curr_slope > 0:\n",
    "            pivot_idx = i + window // 2\n",
    "            if 0 <= pivot_idx < len(log_prices):\n",
    "                pivots.append({\n",
    "                    'date': dates[pivot_idx],\n",
    "                    'price': regular_prices[pivot_idx],\n",
    "                    'log_price': log_prices[pivot_idx],\n",
    "                    'type': 'low',\n",
    "                    'index': pivot_idx\n",
    "                })\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def detect_derivative_pivots_log(log_prices, regular_prices, dates):\n",
    "    \"\"\"Detect pivots using first and second derivatives ON LOG SCALE\"\"\"\n",
    "    pivots = []\n",
    "\n",
    "    # Calculate first derivative (gradient) ON LOG SCALE\n",
    "    first_deriv = np.gradient(log_prices)\n",
    "\n",
    "    # Calculate second derivative ON LOG SCALE\n",
    "    second_deriv = np.gradient(first_deriv)\n",
    "\n",
    "    for i in range(1, len(log_prices) - 1):\n",
    "        # Look for sign changes in first derivative\n",
    "        if first_deriv[i-1] > 0 and first_deriv[i+1] < 0:  # Peak\n",
    "            pivots.append({\n",
    "                'date': dates[i],\n",
    "                'price': regular_prices[i],\n",
    "                'log_price': log_prices[i],\n",
    "                'type': 'high',\n",
    "                'index': i\n",
    "            })\n",
    "        elif first_deriv[i-1] < 0 and first_deriv[i+1] > 0:  # Trough\n",
    "            pivots.append({\n",
    "                'date': dates[i],\n",
    "                'price': regular_prices[i],\n",
    "                'log_price': log_prices[i],\n",
    "                'type': 'low',\n",
    "                'index': i\n",
    "            })\n",
    "\n",
    "        # Also look for significant second derivative changes (inflection points)\n",
    "        if abs(second_deriv[i]) > np.std(second_deriv) * 2:  # Significant curvature change\n",
    "            if second_deriv[i] < 0:  # Concave down (potential high)\n",
    "                pivots.append({\n",
    "                    'date': dates[i],\n",
    "                    'price': regular_prices[i],\n",
    "                    'log_price': log_prices[i],\n",
    "                    'type': 'high',\n",
    "                    'index': i\n",
    "                })\n",
    "            elif second_deriv[i] > 0:  # Concave up (potential low)\n",
    "                pivots.append({\n",
    "                    'date': dates[i],\n",
    "                    'price': regular_prices[i],\n",
    "                    'log_price': log_prices[i],\n",
    "                    'type': 'low',\n",
    "                    'index': i\n",
    "                })\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def detect_zigzag_pivots_log(log_prices, regular_prices, dates, threshold=0.05):\n",
    "    \"\"\"ZigZag-style pivot detection based on percentage moves ON LOG SCALE\"\"\"\n",
    "    pivots = []\n",
    "\n",
    "    if len(log_prices) < 3:\n",
    "        return pivots\n",
    "\n",
    "    last_pivot_idx = 0\n",
    "    last_pivot_log_price = log_prices[0]\n",
    "    direction = None\n",
    "\n",
    "    for i in range(1, len(log_prices)):\n",
    "        log_price = log_prices[i]\n",
    "        # Calculate percentage change using log difference (more accurate for percentage changes)\n",
    "        pct_change = log_price - last_pivot_log_price  # Log difference = percentage change\n",
    "\n",
    "        if direction is None:\n",
    "            if pct_change > np.log(1 + threshold):  # Convert threshold to log space\n",
    "                direction = 'up'\n",
    "            elif pct_change < np.log(1 - threshold):\n",
    "                direction = 'down'\n",
    "\n",
    "        elif direction == 'up':\n",
    "            if pct_change < np.log(1 - threshold):\n",
    "                pivots.append({\n",
    "                    'date': dates[last_pivot_idx],\n",
    "                    'price': regular_prices[last_pivot_idx],\n",
    "                    'log_price': log_prices[last_pivot_idx],\n",
    "                    'type': 'high',\n",
    "                    'index': last_pivot_idx\n",
    "                })\n",
    "                direction = 'down'\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_log_price = log_price\n",
    "            elif log_price > last_pivot_log_price:\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_log_price = log_price\n",
    "\n",
    "        elif direction == 'down':\n",
    "            if pct_change > np.log(1 + threshold):\n",
    "                pivots.append({\n",
    "                    'date': dates[last_pivot_idx],\n",
    "                    'price': regular_prices[last_pivot_idx],\n",
    "                    'log_price': log_prices[last_pivot_idx],\n",
    "                    'type': 'low',\n",
    "                    'index': last_pivot_idx\n",
    "                })\n",
    "                direction = 'up'\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_log_price = log_price\n",
    "            elif log_price < last_pivot_log_price:\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_log_price = log_price\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def combine_overlapping_pivots(all_pivots, proximity_threshold=3):\n",
    "    \"\"\"Combine pivots that are close to each other with improved logic\"\"\"\n",
    "    if not all_pivots:\n",
    "        return []\n",
    "\n",
    "    # Sort by index\n",
    "    all_pivots.sort(key=lambda x: x['index'])\n",
    "\n",
    "    combined = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(all_pivots):\n",
    "        current_pivot = all_pivots[i]\n",
    "        group = [current_pivot]\n",
    "\n",
    "        # Look ahead for similar pivots\n",
    "        j = i + 1\n",
    "        while j < len(all_pivots):\n",
    "            next_pivot = all_pivots[j]\n",
    "\n",
    "            # Same type and within proximity\n",
    "            if (next_pivot['type'] == current_pivot['type'] and\n",
    "                abs(next_pivot['index'] - current_pivot['index']) <= proximity_threshold):\n",
    "                group.append(next_pivot)\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Choose the best pivot from the group based on LOG PRICES\n",
    "        if len(group) == 1:\n",
    "            combined.append(group[0])\n",
    "        else:\n",
    "            # For highs, take the highest LOG price; for lows, take the lowest LOG price\n",
    "            if current_pivot['type'] == 'high':\n",
    "                best_pivot = max(group, key=lambda x: x['log_price'])\n",
    "            else:\n",
    "                best_pivot = min(group, key=lambda x: x['log_price'])\n",
    "\n",
    "            # If multiple have same log price, take the one with highest strength\n",
    "            same_price_group = [p for p in group if abs(p['log_price'] - best_pivot['log_price']) < 1e-6]\n",
    "            if len(same_price_group) > 1:\n",
    "                best_pivot = max(same_price_group, key=lambda x: x.get('strength', 1))\n",
    "\n",
    "            combined.append(best_pivot)\n",
    "\n",
    "        i = j\n",
    "\n",
    "    return combined\n",
    "\n",
    "def get_indices_by_type(pivots, pivot_type):\n",
    "    \"\"\"Extract indices for a specific pivot type\"\"\"\n",
    "    return np.array([p['index'] for p in pivots if p['type'] == pivot_type])\n",
    "\n",
    "def safe_date_format(date_obj):\n",
    "    \"\"\"Safely format date object to string\"\"\"\n",
    "    if hasattr(date_obj, 'strftime'):\n",
    "        return date_obj.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        # Handle numpy datetime64 or other formats\n",
    "        return pd.to_datetime(date_obj).strftime('%Y-%m-%d')\n",
    "\n",
    "# Ultra-enhanced detection ON LOG SCALE\n",
    "pivots, swing_highs, swing_lows = detect_pivot_points_ultra_log(stock_data)\n",
    "\n",
    "# Enhanced visualization with LOG SCALE and better colors\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# ðŸ”¬ PLOT WITH LOG SCALE ON Y-AXIS\n",
    "plt.semilogy(stock_data['Date'], stock_data['Price'], 'k-', alpha=0.8, linewidth=1.5, label=f'{SYMBOL} Price (Log Scale)')\n",
    "\n",
    "# Updated color map for better visibility on white background\n",
    "method_colors = {\n",
    "    'scipy': 'red',\n",
    "    'rolling': 'darkorange',\n",
    "    'zigzag': 'gold',\n",
    "    'fractal': 'green',\n",
    "    'slope': 'blue',\n",
    "    'derivative': 'purple'\n",
    "}\n",
    "\n",
    "# Plot pivots with different colors and sizes based on method and strength\n",
    "for pivot in pivots:\n",
    "    method_type = pivot['method'].split('_')[0]\n",
    "    color = method_colors.get(method_type, 'black')\n",
    "\n",
    "    marker = '^' if pivot['type'] == 'high' else 'v'\n",
    "    strength = pivot.get('strength', 1)\n",
    "    size = max(40, min(100, strength * 15))  # Size based on strength\n",
    "    alpha = max(0.6, min(1.0, strength / 10))\n",
    "\n",
    "    plt.scatter([pivot['date']], [pivot['price']],\n",
    "               color=color, marker=marker, s=size, alpha=alpha,\n",
    "               edgecolors='black', linewidths=0.5, zorder=5)\n",
    "\n",
    "# Create comprehensive legend\n",
    "legend_elements = []\n",
    "for method, color in method_colors.items():\n",
    "    legend_elements.append(plt.scatter([], [], color=color, marker='^', s=60,\n",
    "                                     label=f'{method.title()}', alpha=0.8))\n",
    "\n",
    "plt.title(f'{SYMBOL} - Ultra-Enhanced Pivot Detection (6 Methods) - LOG SCALE', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price ($) - LOG SCALE', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Ultra-Enhanced LOG SCALE Pivot Points Summary:\")\n",
    "print(f\"   Total pivots: {len(pivots)}\")\n",
    "print(f\"   Swing highs: {len(swing_highs)}\")\n",
    "print(f\"   Swing lows: {len(swing_lows)}\")\n",
    "\n",
    "# Method breakdown\n",
    "method_breakdown = {}\n",
    "for pivot in pivots:\n",
    "    method = pivot['method'].split('_')[0]\n",
    "    method_breakdown[method] = method_breakdown.get(method, 0) + 1\n",
    "\n",
    "print(f\"   Method breakdown: {method_breakdown}\")\n",
    "\n",
    "# Show density of detection\n",
    "if len(stock_data) > 0:\n",
    "    pivot_density = len(pivots) / len(stock_data) * 100\n",
    "    print(f\"   Pivot density: {pivot_density:.1f}% of price points are pivots\")\n",
    "\n",
    "# Show recent pivots for validation (with safe date formatting)\n",
    "recent_pivots = sorted([p for p in pivots], key=lambda x: x['date'])[-10:]\n",
    "print(f\"\\nðŸ“ˆ Most Recent 10 LOG SCALE Pivots:\")\n",
    "for i, pivot in enumerate(recent_pivots):\n",
    "    date_str = safe_date_format(pivot['date'])\n",
    "    print(f\"   {i+1}: ${pivot['price']:.2f} (log: {pivot['log_price']:.4f}) ({pivot['type']}) - {date_str} - {pivot['method']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:21.114325Z",
     "iopub.status.busy": "2025-09-04T03:30:21.114190Z",
     "iopub.status.idle": "2025-09-04T03:30:21.233887Z",
     "shell.execute_reply": "2025-09-04T03:30:21.233503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Detect powerful LOG SCALE trendlines through iterative best-fit refinement\n",
    "def find_iterative_trendline_log(pivot1, pivot2, all_pivots, tolerance_percent=2.0):  # ðŸ”§ USE PERCENTAGE\n",
    "    \"\"\"\n",
    "    Iteratively refine trendline using LOG SCALE by:\n",
    "    1. Start with 2 points\n",
    "    2. Find best-fit line using LOG PRICES\n",
    "    3. Find other points within tolerance\n",
    "    4. Add them and recalculate best-fit line\n",
    "    5. Repeat until no new points found within tolerance\n",
    "    \"\"\"\n",
    "    # Start with the initial two points\n",
    "    current_points = [pivot1, pivot2]\n",
    "\n",
    "    # Convert to numerical format for calculations using LOG SCALE\n",
    "    def points_to_xy_log(points):\n",
    "        x_vals = [(p['date'] - stock_data['Date'].iloc[0]).days for p in points]\n",
    "        y_vals = [p['log_price'] for p in points]  # ðŸ”¬ USE LOG PRICES for trendline fitting\n",
    "        return x_vals, y_vals\n",
    "\n",
    "    # ðŸ”§ CONVERT PERCENTAGE TO LOG TOLERANCE\n",
    "    log_tolerance = np.log(1 + tolerance_percent/100)  # Convert 2% to proper log tolerance\n",
    "\n",
    "    max_iterations = 100  # Allow more iterations for better fitting\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "\n",
    "        # Calculate current best-fit line using LOG PRICES\n",
    "        x_vals, y_vals = points_to_xy_log(current_points)\n",
    "\n",
    "        if len(x_vals) < 2:\n",
    "            break\n",
    "\n",
    "        # Use scipy.stats.linregress for best-fit line on LOG SCALE\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x_vals, y_vals)\n",
    "\n",
    "        # Find additional points within tolerance of this best-fit line\n",
    "        new_points = []\n",
    "        for pivot in all_pivots:\n",
    "            # Skip if already in current_points\n",
    "            if pivot in current_points:\n",
    "                continue\n",
    "\n",
    "            x_pivot = (pivot['date'] - stock_data['Date'].iloc[0]).days\n",
    "            expected_log_y = slope * x_pivot + intercept  # Expected LOG price\n",
    "            actual_log_y = pivot['log_price']             # Actual LOG price\n",
    "\n",
    "            # ðŸ”§ PROPER LOG TOLERANCE: Convert percentage tolerance to log space\n",
    "            log_difference = abs(expected_log_y - actual_log_y)\n",
    "            if log_difference <= log_tolerance:  # Proper percentage tolerance in log space\n",
    "                new_points.append(pivot)\n",
    "\n",
    "        # If no new points found, we're done\n",
    "        if not new_points:\n",
    "            break\n",
    "\n",
    "        # Add new points and continue iteration\n",
    "        current_points.extend(new_points)\n",
    "\n",
    "    # Final calculation with all points using LOG SCALE\n",
    "    if len(current_points) >= 2:\n",
    "        x_vals, y_vals = points_to_xy_log(current_points)\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x_vals, y_vals)\n",
    "        r_squared = r_value ** 2\n",
    "\n",
    "        # Calculate percentage growth rate from log slope\n",
    "        daily_growth_rate = (np.exp(slope) - 1) * 100  # Convert log slope to percentage growth per day\n",
    "\n",
    "        return {\n",
    "            'connected_points': current_points,\n",
    "            'strength': len(current_points),\n",
    "            'log_slope': slope,          # Log slope for calculations\n",
    "            'log_intercept': intercept,  # Log intercept for calculations\n",
    "            'daily_growth_rate': daily_growth_rate,  # Percentage growth rate per day\n",
    "            'r_squared': r_squared,\n",
    "            'iterations': iteration\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def detect_powerful_trendlines_log(pivots, max_lines=30):\n",
    "    \"\"\"Find powerful LOG SCALE trendlines using iterative best-fit refinement with smart pair removal\"\"\"\n",
    "    trendlines = []\n",
    "    used_trendline_pairs = set()  # ðŸ”§ Track pairs that are BOTH in the same trendline\n",
    "\n",
    "    print(f\"ðŸ” LOG SCALE iterative trendline detection with proper 2% tolerance...\")\n",
    "\n",
    "    # Create list of all possible pairs first\n",
    "    all_pairs = []\n",
    "    for i, pivot1 in enumerate(pivots):\n",
    "        for j, pivot2 in enumerate(pivots[i+1:], i+1):\n",
    "            # ðŸ”§ REMOVED TIME CONSTRAINT - no minimum time separation required\n",
    "            all_pairs.append((i, j, pivot1, pivot2))\n",
    "\n",
    "    print(f\"   Created {len(all_pairs)} potential trendline pairs (no time constraints)\")\n",
    "\n",
    "    # Sort pairs by time distance to prefer longer trendlines first\n",
    "    all_pairs.sort(key=lambda x: abs((x[3]['date'] - x[2]['date']).days), reverse=True)\n",
    "\n",
    "    processed_pairs = 0\n",
    "    skipped_pairs = 0\n",
    "\n",
    "    for i, j, pivot1, pivot2 in all_pairs:\n",
    "        processed_pairs += 1\n",
    "\n",
    "        # ðŸ”§ SMART PAIR REMOVAL: Skip only if BOTH points are in the same existing trendline\n",
    "        pair_key = tuple(sorted([i, j]))\n",
    "        if pair_key in used_trendline_pairs:\n",
    "            skipped_pairs += 1\n",
    "            continue\n",
    "\n",
    "        # Find iterative trendline starting with this pair using LOG SCALE\n",
    "        result = find_iterative_trendline_log(pivot1, pivot2, pivots, tolerance_percent=2.0)  # 2% proper tolerance\n",
    "\n",
    "        if result and result['strength'] >= 2:  # Must connect at least 2 points\n",
    "            trendline = {\n",
    "                'start_pivot': pivot1,\n",
    "                'end_pivot': pivot2,\n",
    "                'connected_points': result['connected_points'],\n",
    "                'strength': result['strength'],\n",
    "                'log_slope': result['log_slope'],           # Store log slope\n",
    "                'log_intercept': result['log_intercept'],   # Store log intercept\n",
    "                'daily_growth_rate': result['daily_growth_rate'],  # Percentage growth per day\n",
    "                'r_squared': result['r_squared'],\n",
    "                'iterations': result['iterations'],\n",
    "                'length_days': abs((pivot2['date'] - pivot1['date']).days)\n",
    "            }\n",
    "\n",
    "            trendlines.append(trendline)\n",
    "\n",
    "            # ðŸ”§ SMART PAIR REMOVAL: Only remove pairs where BOTH points are in this trendline\n",
    "            connected_indices = []\n",
    "            for point in result['connected_points']:\n",
    "                try:\n",
    "                    idx = next(idx for idx, p in enumerate(pivots) if p == point)\n",
    "                    connected_indices.append(idx)\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "\n",
    "            # Remove pairs that use ANY TWO points from this trendline's connected points\n",
    "            new_removed_pairs = 0\n",
    "            for pi in range(len(connected_indices)):\n",
    "                for pj in range(pi + 1, len(connected_indices)):\n",
    "                    pair_to_remove = tuple(sorted([connected_indices[pi], connected_indices[pj]]))\n",
    "                    if pair_to_remove not in used_trendline_pairs:\n",
    "                        used_trendline_pairs.add(pair_to_remove)\n",
    "                        new_removed_pairs += 1\n",
    "\n",
    "            if len(trendlines) <= 10:  # Only show details for first 10\n",
    "                print(f\"   Found LOG trendline #{len(trendlines)}: {result['strength']} points, RÂ²={result['r_squared']:.3f}, growth={result['daily_growth_rate']:.4f}%/day, {result['iterations']} iterations\")\n",
    "                print(f\"      Removed {new_removed_pairs} internal pairs from future searches\")\n",
    "\n",
    "            # Stop if we have enough trendlines\n",
    "            if len(trendlines) >= max_lines:\n",
    "                break\n",
    "\n",
    "    # Sort by strength and R-squared\n",
    "    trendlines.sort(key=lambda x: (x['strength'], x['r_squared']), reverse=True)\n",
    "\n",
    "    # Take top max_lines\n",
    "    top_trendlines = trendlines[:max_lines]\n",
    "\n",
    "    print(f\"âœ… Found {len(trendlines)} valid LOG SCALE trendlines using iterative refinement\")\n",
    "    print(f\"   Processed {processed_pairs} pairs, skipped {skipped_pairs} internal pairs\")\n",
    "    print(f\"   Final selection: {len(top_trendlines)} trendlines\")\n",
    "\n",
    "    if top_trendlines:\n",
    "        strengths = [tl['strength'] for tl in top_trendlines]\n",
    "        growth_rates = [tl['daily_growth_rate'] for tl in top_trendlines]\n",
    "        iterations = [tl['iterations'] for tl in top_trendlines]\n",
    "\n",
    "        print(f\"   Strength range: {min(strengths)} - {max(strengths)} connected points\")\n",
    "        print(f\"   Average strength: {np.mean(strengths):.1f} connected points\")\n",
    "        print(f\"   Growth rate range: {min(growth_rates):.4f}% - {max(growth_rates):.4f}% per day\")\n",
    "        print(f\"   Average growth rate: {np.mean(growth_rates):.4f}% per day\")\n",
    "        print(f\"   Average iterations: {np.mean(iterations):.1f}\")\n",
    "\n",
    "    return top_trendlines\n",
    "\n",
    "# For backwards compatibility, keep the old function signature\n",
    "def calculate_trendline_strength_log(pivot1, pivot2, all_pivots, tolerance_percent=2.0):\n",
    "    \"\"\"Wrapper for backwards compatibility\"\"\"\n",
    "    result = find_iterative_trendline_log(pivot1, pivot2, all_pivots, tolerance_percent)\n",
    "    if result:\n",
    "        return result['strength'], result['connected_points']\n",
    "    else:\n",
    "        return 0, []\n",
    "\n",
    "powerful_trendlines = detect_powerful_trendlines_log(pivots, MAX_TRENDLINES)\n",
    "\n",
    "# Display LOG SCALE trendline statistics\n",
    "if powerful_trendlines:\n",
    "    trendline_df = pd.DataFrame([\n",
    "        {\n",
    "            'Strength': tl['strength'],\n",
    "            'RÂ²': f\"{tl['r_squared']:.3f}\",\n",
    "            'Growth %/day': f\"{tl['daily_growth_rate']:.4f}%\",\n",
    "            'Log Slope': f\"{tl['log_slope']:.6f}\",\n",
    "            'Iterations': tl['iterations'],\n",
    "            'Length (days)': tl['length_days']\n",
    "        } for tl in powerful_trendlines[:15]  # Show top 15\n",
    "    ])\n",
    "    print(f\"\\nðŸ“ˆ Top 15 LOG SCALE Iteratively Refined Trendlines (proper 2% tolerance, no time constraints):\")\n",
    "    print(trendline_df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\nðŸ”¬ LOG SCALE Trendline Interpretation:\")\n",
    "    print(f\"   â€¢ Growth %/day: Daily percentage growth rate (compound interest)\")\n",
    "    print(f\"   â€¢ Tolerance: 2% price deviation (properly converted to log space)\")\n",
    "    print(f\"   â€¢ No time constraints: Any pivot separation allowed\")\n",
    "    print(f\"   â€¢ Smart pair removal: Points can be reused in different trendlines\")\n",
    "else:\n",
    "    print(\"âš ï¸ No powerful LOG SCALE trendlines found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:21.235890Z",
     "iopub.status.busy": "2025-09-04T03:30:21.235733Z",
     "iopub.status.idle": "2025-09-04T03:30:21.542911Z",
     "shell.execute_reply": "2025-09-04T03:30:21.542552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Visualize LOG SCALE trendlines with EXTENDED projections and thickness based on strength\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# ðŸ”¬ PLOT WITH LOG SCALE ON Y-AXIS\n",
    "plt.semilogy(stock_data['Date'], stock_data['Price'], 'k-', alpha=0.7, linewidth=1, label=f'{SYMBOL} Price (Log Scale)')\n",
    "\n",
    "# Plot pivot points\n",
    "plt.scatter(stock_data['Date'].iloc[swing_highs], stock_data['Price'].iloc[swing_highs],\n",
    "           color='red', marker='^', s=30, alpha=0.7, label='Swing Highs')\n",
    "plt.scatter(stock_data['Date'].iloc[swing_lows], stock_data['Price'].iloc[swing_lows],\n",
    "           color='green', marker='v', s=30, alpha=0.7, label='Swing Lows')\n",
    "\n",
    "# Plot LOG SCALE trendlines with EXTENDED projections and thickness based on strength\n",
    "if powerful_trendlines:\n",
    "    # Calculate strength-based thickness\n",
    "    strengths = [tl['strength'] for tl in powerful_trendlines]\n",
    "    min_strength = min(strengths)\n",
    "    max_strength = max(strengths)\n",
    "\n",
    "    # Define thickness range (from 1 to 5 pixels)\n",
    "    min_thickness = 1.0\n",
    "    max_thickness = 5.0\n",
    "\n",
    "    print(f\"ðŸ“Š LOG SCALE Trendline thickness mapping:\")\n",
    "    print(f\"   Strength range: {min_strength} - {max_strength} points\")\n",
    "    print(f\"   Thickness range: {min_thickness} - {max_thickness} pixels\")\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(powerful_trendlines)))\n",
    "\n",
    "    # Calculate extended projection dates\n",
    "    current_date = stock_data['Date'].iloc[-1]\n",
    "\n",
    "    # ðŸŽ¯ EXTEND PROJECTIONS MUCH FURTHER: Show 30 days ahead for better visualization\n",
    "    extended_projection_days = 30\n",
    "    extended_end_date = current_date + timedelta(days=extended_projection_days)\n",
    "\n",
    "    print(f\"ðŸ“ˆ EXTENDED PROJECTIONS: Showing {extended_projection_days} days ahead\")\n",
    "    print(f\"   Current date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Extended to: {extended_end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    for i, trendline in enumerate(powerful_trendlines):\n",
    "        start_date = trendline['start_pivot']['date']\n",
    "        end_date = trendline['end_pivot']['date']\n",
    "\n",
    "        # ðŸš€ EXTEND ALL TRENDLINES TO RIGHT EDGE: Use extended projection end date\n",
    "        final_projection_date = extended_end_date\n",
    "\n",
    "        # Calculate trendline values using LOG SCALE for ENTIRE extended range\n",
    "        x_start = (start_date - stock_data['Date'].iloc[0]).days\n",
    "        x_current = (current_date - stock_data['Date'].iloc[0]).days\n",
    "        x_extended = (final_projection_date - stock_data['Date'].iloc[0]).days\n",
    "\n",
    "        # Calculate LOG values first, then convert to regular prices for plotting\n",
    "        log_y_start = trendline['log_slope'] * x_start + trendline['log_intercept']\n",
    "        log_y_current = trendline['log_slope'] * x_current + trendline['log_intercept']\n",
    "        log_y_extended = trendline['log_slope'] * x_extended + trendline['log_intercept']\n",
    "\n",
    "        # Convert back to regular prices for plotting\n",
    "        y_start = np.exp(log_y_start)\n",
    "        y_current = np.exp(log_y_current)\n",
    "        y_extended = np.exp(log_y_extended)\n",
    "\n",
    "        # Calculate thickness based on strength (relative to min/max)\n",
    "        if max_strength > min_strength:\n",
    "            strength_ratio = (trendline['strength'] - min_strength) / (max_strength - min_strength)\n",
    "        else:\n",
    "            strength_ratio = 1.0\n",
    "\n",
    "        line_thickness = min_thickness + strength_ratio * (max_thickness - min_thickness)\n",
    "\n",
    "        # ðŸŽ¨ PLOT HISTORICAL PART (solid line from start to current)\n",
    "        plt.plot([start_date, current_date],\n",
    "                [y_start, y_current],\n",
    "                color=colors[i], linewidth=line_thickness, alpha=0.8,\n",
    "                label=f'Line {i+1} ({trendline[\"strength\"]} pts, {trendline[\"daily_growth_rate\"]:.3f}%/day)' if i < 5 else \"\")\n",
    "\n",
    "        # ðŸš€ PLOT EXTENDED PROJECTION (dashed line from current to extended end)\n",
    "        plt.plot([current_date, final_projection_date],\n",
    "                [y_current, y_extended],\n",
    "                color=colors[i], linewidth=line_thickness * 0.8, alpha=0.6, linestyle='--')\n",
    "\n",
    "        # Add strength and growth rate annotation for top 5 strongest lines\n",
    "        if i < 5:\n",
    "            # Position annotation in the projection area for better visibility\n",
    "            annotation_date = current_date + timedelta(days=extended_projection_days * 0.3)  # 30% into projection\n",
    "            annotation_x = (annotation_date - stock_data['Date'].iloc[0]).days\n",
    "            log_annotation_y = trendline['log_slope'] * annotation_x + trendline['log_intercept']\n",
    "            annotation_y = np.exp(log_annotation_y)\n",
    "\n",
    "            # Create annotation text with both strength and growth rate\n",
    "            annotation_text = f\"{trendline['strength']}pts\\n{trendline['daily_growth_rate']:.3f}%/d\"\n",
    "\n",
    "            plt.annotate(annotation_text,\n",
    "                        xy=(annotation_date, annotation_y),\n",
    "                        xytext=(8, 8), textcoords='offset points',\n",
    "                        fontsize=8, fontweight='bold', color=colors[i],\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9,\n",
    "                                edgecolor=colors[i], linewidth=1))\n",
    "\n",
    "    # ðŸŽ¯ ADD VERTICAL LINE TO MARK CURRENT DATE\n",
    "    plt.axvline(x=current_date, color='orange', linestyle='-', linewidth=2, alpha=0.8,\n",
    "                label='Current Date', zorder=10)\n",
    "\n",
    "    plt.title(f'{SYMBOL} - Top {len(powerful_trendlines)} LOG SCALE Trendlines (EXTENDED {extended_projection_days}d Projections)\\n'\n",
    "              f'Thickness = Strength | Dashed = Projections | Solid = Historical',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Price ($) - LOG SCALE', fontsize=12)\n",
    "\n",
    "    # Create comprehensive legend with extended projection info\n",
    "    legend_elements = []\n",
    "    legend_elements.append(plt.Line2D([0], [0], color='black', lw=1, label=f'{SYMBOL} Price (Log Scale)'))\n",
    "    legend_elements.append(plt.scatter([], [], color='red', marker='^', s=30, label='Swing Highs'))\n",
    "    legend_elements.append(plt.scatter([], [], color='green', marker='v', s=30, label='Swing Lows'))\n",
    "    legend_elements.append(plt.Line2D([0], [0], color='orange', lw=2, label='Current Date'))\n",
    "\n",
    "    # Add thickness and growth rate examples to legend\n",
    "    if len(powerful_trendlines) > 0:\n",
    "        strongest = max(powerful_trendlines, key=lambda x: x['strength'])\n",
    "        weakest = min(powerful_trendlines, key=lambda x: x['strength'])\n",
    "\n",
    "        # Find most bullish and bearish trendlines\n",
    "        most_bullish = max(powerful_trendlines, key=lambda x: x['daily_growth_rate'])\n",
    "        most_bearish = min(powerful_trendlines, key=lambda x: x['daily_growth_rate'])\n",
    "\n",
    "        legend_elements.append(plt.Line2D([0], [0], color='purple', lw=max_thickness,\n",
    "                                        label=f'Strongest ({strongest[\"strength\"]} pts)'))\n",
    "        legend_elements.append(plt.Line2D([0], [0], color='purple', lw=min_thickness,\n",
    "                                        label=f'Weakest ({weakest[\"strength\"]} pts)'))\n",
    "        legend_elements.append(plt.Line2D([0], [0], color='green', lw=3,\n",
    "                                        label=f'Most Bullish ({most_bullish[\"daily_growth_rate\"]:.3f}%/day)'))\n",
    "        legend_elements.append(plt.Line2D([0], [0], color='red', lw=3,\n",
    "                                        label=f'Most Bearish ({most_bearish[\"daily_growth_rate\"]:.3f}%/day)'))\n",
    "        legend_elements.append(plt.Line2D([0], [0], color='gray', lw=2, linestyle='--',\n",
    "                                        label=f'Extended Projections ({extended_projection_days}d)'))\n",
    "\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Enhanced statistics with projection analysis\n",
    "    print(f\"\\nðŸ“Š LOG SCALE Trendline Strength & Growth Distribution:\")\n",
    "\n",
    "    # Group by strength and show growth rates\n",
    "    strength_growth = {}\n",
    "    for tl in powerful_trendlines:\n",
    "        strength = tl['strength']\n",
    "        growth_rate = tl['daily_growth_rate']\n",
    "        if strength not in strength_growth:\n",
    "            strength_growth[strength] = []\n",
    "        strength_growth[strength].append(growth_rate)\n",
    "\n",
    "    for strength in sorted(strength_growth.keys(), reverse=True):\n",
    "        growth_rates = strength_growth[strength]\n",
    "        count = len(growth_rates)\n",
    "        avg_growth = np.mean(growth_rates)\n",
    "        thickness = min_thickness + ((strength - min_strength) / max(1, max_strength - min_strength)) * (max_thickness - min_thickness)\n",
    "        print(f\"   {strength} points: {count} lines (thickness: {thickness:.1f}px, avg growth: {avg_growth:.3f}%/day)\")\n",
    "\n",
    "    # Show growth rate statistics\n",
    "    growth_rates = [tl['daily_growth_rate'] for tl in powerful_trendlines]\n",
    "    print(f\"\\nðŸ“ˆ Growth Rate Analysis:\")\n",
    "    print(f\"   Range: {min(growth_rates):.3f}% to {max(growth_rates):.3f}% per day\")\n",
    "    print(f\"   Average: {np.mean(growth_rates):.3f}% per day\")\n",
    "    print(f\"   Bullish lines (>0%): {len([g for g in growth_rates if g > 0])}\")\n",
    "    print(f\"   Bearish lines (<0%): {len([g for g in growth_rates if g < 0])}\")\n",
    "\n",
    "    # ðŸš€ PROJECTION ANALYSIS: Show where trendlines will be in 30 days\n",
    "    print(f\"\\nðŸš€ EXTENDED PROJECTION ANALYSIS ({extended_projection_days} days ahead):\")\n",
    "    current_price = stock_data['Price'].iloc[-1]\n",
    "\n",
    "    projection_prices = []\n",
    "    for i, tl in enumerate(powerful_trendlines[:10]):  # Show top 10\n",
    "        x_future = (extended_end_date - stock_data['Date'].iloc[0]).days\n",
    "        log_future_price = tl['log_slope'] * x_future + tl['log_intercept']\n",
    "        future_price = np.exp(log_future_price)\n",
    "\n",
    "        price_change = ((future_price - current_price) / current_price) * 100\n",
    "        projection_prices.append(future_price)\n",
    "\n",
    "        print(f\"   TL{i+1}: ${future_price:.2f} ({price_change:+.1f}% from current)\")\n",
    "\n",
    "    # Show projection range\n",
    "    if projection_prices:\n",
    "        print(f\"\\nðŸ“Š Projection Range Summary:\")\n",
    "        print(f\"   Current price: ${current_price:.2f}\")\n",
    "        print(f\"   Projected range: ${min(projection_prices):.2f} - ${max(projection_prices):.2f}\")\n",
    "        print(f\"   Average projection: ${np.mean(projection_prices):.2f}\")\n",
    "\n",
    "    # Convert to annual rates for context\n",
    "    annual_rates = [(np.exp(g/100 * 365) - 1) * 100 for g in growth_rates]\n",
    "    print(f\"\\nðŸ“… Annualized Growth Rates (compound):\")\n",
    "    print(f\"   Range: {min(annual_rates):.1f}% to {max(annual_rates):.1f}% per year\")\n",
    "    print(f\"   Average: {np.mean(annual_rates):.1f}% per year\")\n",
    "\n",
    "else:\n",
    "    plt.title(f'{SYMBOL} - No Powerful LOG SCALE Trendlines Found', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Price ($) - LOG SCALE', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Enhanced LOG SCALE Visualization with Extended Projections:\")\n",
    "if powerful_trendlines:\n",
    "    print(f\"   ðŸ”¬ Each trendline represents exponential growth/decay at a constant percentage rate\")\n",
    "    print(f\"   ðŸ“ˆ Positive rates = compound growth, Negative rates = compound decay\")\n",
    "    print(f\"   ðŸ“Š Thickness proportional to strength (number of pivot points connected)\")\n",
    "    print(f\"   ðŸŽ¯ Top 5 strongest lines annotated with strength and daily growth rate\")\n",
    "    print(f\"   ðŸš€ Extended projections show {extended_projection_days}-day forward outlook\")\n",
    "    print(f\"   ðŸŸ  Orange line marks current date (historical vs. projection boundary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T03:30:21.547273Z",
     "iopub.status.busy": "2025-09-04T03:30:21.547123Z",
     "iopub.status.idle": "2025-09-04T03:30:21.562373Z",
     "shell.execute_reply": "2025-09-04T03:30:21.562122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Save LOG SCALE trendlines data with FIRST WINDOW VALIDATION context\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "if VALIDATION_MODE:\n",
    "    print(f\"ðŸ’¾ ðŸŽ¯ Saving LOG SCALE trendlines for FIRST WINDOW VALIDATION...\")\n",
    "    print(f\"   ðŸ“… Window: {WINDOW_START} to {WINDOW_END} (W000)\")\n",
    "    print(f\"   ðŸŽª Expected clusters from analyzer: {EXPECTED_CLUSTERS}\")\n",
    "    print(f\"   ðŸ’° Expected final price: ${EXPECTED_PRICE}\")\n",
    "else:\n",
    "    print(f\"ðŸ’¾ Saving LOG SCALE trendline data for trend cloud analysis...\")\n",
    "\n",
    "try:\n",
    "    # Save the LOG SCALE trendlines with validation context\n",
    "    trendline_data = {\n",
    "        'powerful_trendlines': powerful_trendlines,\n",
    "        'stock_data': stock_data,\n",
    "        'pivots': pivots,\n",
    "        'symbol': SYMBOL,\n",
    "        'projection_days': PROJECTION_DAYS,\n",
    "        'temperature': TEMPERATURE,\n",
    "        'log_scale': True,  # ðŸ”¬ FLAG TO INDICATE LOG SCALE DATA\n",
    "        'analysis_type': 'log_scale',\n",
    "        'validation_mode': VALIDATION_MODE,  # ðŸŽ¯ VALIDATION FLAG\n",
    "    }\n",
    "\n",
    "    # Add validation-specific data\n",
    "    if VALIDATION_MODE:\n",
    "        trendline_data['validation_context'] = {\n",
    "            'window_id': 'W000',\n",
    "            'window_start': WINDOW_START,\n",
    "            'window_end': WINDOW_END,\n",
    "            'expected_price': EXPECTED_PRICE,\n",
    "            'expected_clusters': EXPECTED_CLUSTERS,\n",
    "            'actual_price': float(stock_data['Price'].iloc[-1]),\n",
    "            'price_difference': abs(float(stock_data['Price'].iloc[-1]) - EXPECTED_PRICE),\n",
    "            'validation_purpose': 'First window validation against fixed clustering algorithm'\n",
    "        }\n",
    "\n",
    "    # Choose filename based on mode\n",
    "    if VALIDATION_MODE:\n",
    "        pickle_filename = 'data/trendlines_data_log_validation_w000.pkl'\n",
    "        json_filename = 'data/trendlines_summary_log_validation_w000.json'\n",
    "    else:\n",
    "        pickle_filename = 'data/trendlines_data_log.pkl'\n",
    "        json_filename = 'data/trendlines_summary_log.json'\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump(trendline_data, f)\n",
    "\n",
    "    print(f\"âœ… Saved LOG SCALE trendline data to {pickle_filename}\")\n",
    "    print(f\"   ðŸŽ¯ {len(powerful_trendlines)} powerful LOG SCALE trendlines\")\n",
    "    print(f\"   ðŸ“Š {len(stock_data)} stock data points with LogPrice column\")\n",
    "    print(f\"   ðŸ“ {len(pivots)} LOG SCALE pivot points\")\n",
    "    print(f\"   ðŸ’¹ Symbol: {SYMBOL}\")\n",
    "    print(f\"   ðŸ“… Date range: {stock_data['Date'].min().date()} to {stock_data['Date'].max().date()}\")\n",
    "    print(f\"   ðŸ’° Current price: ${stock_data['Price'].iloc[-1]:.2f} (log: {stock_data['LogPrice'].iloc[-1]:.4f})\")\n",
    "\n",
    "    # Also save as JSON for easier inspection with validation context\n",
    "    json_data = {\n",
    "        'symbol': SYMBOL,\n",
    "        'projection_days': PROJECTION_DAYS,\n",
    "        'temperature': TEMPERATURE,\n",
    "        'current_price': float(stock_data['Price'].iloc[-1]),\n",
    "        'current_log_price': float(stock_data['LogPrice'].iloc[-1]),\n",
    "        'log_scale': True,\n",
    "        'analysis_type': 'log_scale',\n",
    "        'validation_mode': VALIDATION_MODE,\n",
    "        'date_range': {\n",
    "            'start': stock_data['Date'].min().strftime('%Y-%m-%d'),\n",
    "            'end': stock_data['Date'].max().strftime('%Y-%m-%d')\n",
    "        },\n",
    "        'trendlines': []\n",
    "    }\n",
    "\n",
    "    # Add validation context to JSON\n",
    "    if VALIDATION_MODE:\n",
    "        json_data['validation_context'] = trendline_data['validation_context']\n",
    "        json_data['validation_summary'] = {\n",
    "            'window_period_days': len(stock_data),\n",
    "            'price_validation_passed': abs(float(stock_data['Price'].iloc[-1]) - EXPECTED_PRICE) < 1.0,\n",
    "            'ready_for_clustering_comparison': True,\n",
    "            'next_step': 'Run 1.1 trend_cloud_analysis.ipynb for clustering validation'\n",
    "        }\n",
    "\n",
    "    for i, tl in enumerate(powerful_trendlines):\n",
    "        trendline_data_item = {\n",
    "            'id': i,\n",
    "            'strength': int(tl['strength']),\n",
    "            'log_slope': float(tl['log_slope']),\n",
    "            'log_intercept': float(tl['log_intercept']),\n",
    "            'daily_growth_rate': float(tl['daily_growth_rate']),\n",
    "            'annual_growth_rate': float((np.exp(tl['daily_growth_rate']/100 * 365) - 1) * 100),\n",
    "            'r_squared': float(tl['r_squared']),\n",
    "            'iterations': int(tl['iterations']),\n",
    "            'length_days': int(tl['length_days']),\n",
    "            'start_date': tl['start_pivot']['date'].strftime('%Y-%m-%d'),\n",
    "            'start_price': float(tl['start_pivot']['price']),\n",
    "            'start_log_price': float(tl['start_pivot']['log_price']),\n",
    "            'end_date': tl['end_pivot']['date'].strftime('%Y-%m-%d'),\n",
    "            'end_price': float(tl['end_pivot']['price']),\n",
    "            'end_log_price': float(tl['end_pivot']['log_price'])\n",
    "        }\n",
    "\n",
    "        json_data['trendlines'].append(trendline_data_item)\n",
    "\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(f\"âœ… Also saved LOG SCALE summary to {json_filename} for inspection\")\n",
    "\n",
    "    strengths = [tl['strength'] for tl in powerful_trendlines]\n",
    "    growth_rates = [tl['daily_growth_rate'] for tl in powerful_trendlines]\n",
    "    annual_rates = [(np.exp(g/100 * 365) - 1) * 100 for g in growth_rates]\n",
    "\n",
    "    print(f\"\\nðŸ“‹ LOG SCALE Quick summary:\")\n",
    "    print(f\"   Strength range: {min(strengths)} - {max(strengths)} points\")\n",
    "    print(f\"   Average strength: {sum(strengths)/len(strengths):.1f} points\")\n",
    "    print(f\"   Growth rate range: {min(growth_rates):.3f}% - {max(growth_rates):.3f}% per day\")\n",
    "    print(f\"   Annual growth range: {min(annual_rates):.1f}% - {max(annual_rates):.1f}% per year\")\n",
    "    print(f\"   ðŸ“ˆ Bullish trendlines: {len([g for g in growth_rates if g > 0])}\")\n",
    "    print(f\"   ðŸ“‰ Bearish trendlines: {len([g for g in growth_rates if g < 0])}\")\n",
    "\n",
    "    if VALIDATION_MODE:\n",
    "        print(f\"\\nðŸŽ¯ VALIDATION CONTEXT:\")\n",
    "        print(f\"   Window ID: W000 (First window from fixed analyzer)\")\n",
    "        print(f\"   Period: {WINDOW_START} to {WINDOW_END}\")\n",
    "        print(f\"   Expected vs Actual price: ${EXPECTED_PRICE} vs ${stock_data['Price'].iloc[-1]:.2f}\")\n",
    "        price_diff = abs(float(stock_data['Price'].iloc[-1]) - EXPECTED_PRICE)\n",
    "        if price_diff < 1.0:\n",
    "            print(f\"   âœ… Price validation: PASSED (difference: ${price_diff:.2f})\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ Price validation: Different (difference: ${price_diff:.2f})\")\n",
    "        print(f\"   Ready for clustering validation in 1.1 notebook!\")\n",
    "    else:\n",
    "        print(f\"   Ready to load in LOG SCALE trend cloud analysis notebook!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving LOG SCALE trendlines: {e}\")\n",
    "    print(\"Make sure you have run the previous cells to generate the data\")\n",
    "\n",
    "# Show first few LOG SCALE trendlines as preview with validation context\n",
    "print(f\"\\nðŸ” Preview of saved LOG SCALE trendlines:\")\n",
    "if VALIDATION_MODE:\n",
    "    print(f\"   ðŸŽ¯ Generated from first window (W000) data for validation:\")\n",
    "\n",
    "for i, tl in enumerate(powerful_trendlines[:5]):\n",
    "    annual_growth = (np.exp(tl['daily_growth_rate']/100 * 365) - 1) * 100\n",
    "    print(f\"   TL{i+1}: {tl['strength']} points, growth={tl['daily_growth_rate']:.4f}%/day ({annual_growth:.1f}%/year), RÂ²={tl['r_squared']:.3f}\")\n",
    "    print(f\"        {tl['start_pivot']['date'].strftime('%Y-%m-%d')} ${tl['start_pivot']['price']:.2f} â†’ {tl['end_pivot']['date'].strftime('%Y-%m-%d')} ${tl['end_pivot']['price']:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ LOG SCALE Analysis Complete!\")\n",
    "print(f\"   ðŸ“ˆ Trendlines now represent constant percentage growth rates\")\n",
    "print(f\"   ðŸ’¹ Better suited for exponential price movements\")\n",
    "print(f\"   ðŸŽ¯ More meaningful for long-term trend analysis\")\n",
    "\n",
    "if VALIDATION_MODE:\n",
    "    print(f\"\\nðŸŽ¯ VALIDATION WORKFLOW:\")\n",
    "    print(f\"   1. âœ… First Window Data Loaded (W000: {WINDOW_START} to {WINDOW_END})\")\n",
    "    print(f\"   2. âœ… LOG SCALE Trendlines Generated ({len(powerful_trendlines)} trendlines)\")\n",
    "    print(f\"   3. ðŸ“ Next: Run 1.1 trend_cloud_analysis.ipynb for clustering validation\")\n",
    "    print(f\"   4. ðŸ” Compare clustering results with fixed analyzer expectations:\")\n",
    "    print(f\"      â€¢ Expected clusters: {EXPECTED_CLUSTERS}\")\n",
    "    print(f\"      â€¢ Expected multi-trendline convergence (5-11 trendlines per cluster)\")\n",
    "    print(f\"      â€¢ Validate FIXED clustering algorithm works in both approaches\")\n",
    "\n",
    "    print(f\"\\nðŸ“‹ Files saved for validation:\")\n",
    "    print(f\"   â€¢ {pickle_filename}\")\n",
    "    print(f\"   â€¢ {json_filename}\")\n",
    "    print(f\"   â€¢ Ready for trend cloud clustering validation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
